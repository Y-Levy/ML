# -*- coding: utf-8 -*-
"""ML_PROJ_204382949_305730897.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QtxUGC7EaZ-oOR2xRkVKkEmppVtoir-i
"""

data_path = 'content/hribm.csv'

"""# Data understanding"""

import numpy as np
import seaborn as sns
from sklearn.model_selection import KFold, cross_val_score
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn import preprocessing
from sklearn import metrics
from sklearn.metrics import mean_squared_error
from sklearn.metrics import explained_variance_score
from sklearn.metrics import r2_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.datasets import make_regression
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split 
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_validate
from sklearn.metrics import accuracy_score, confusion_matrix
#import matplotlib.transforms as mtransforms

df_dataset = pd.read_csv(data_path)


np.random.seed(42)

dataDescribe = df_dataset.describe(include='all')
print(dataDescribe)

"""Graphs

*MonthlyIncome*
"""

sns.distplot(df_dataset['MonthlyIncome'],  hist=False, kde=True, color='purple',norm_hist=True).set(title='MonthlyIncome Density')
plt.show()
#scatter
plt.scatter(x=df_dataset.index, y=df_dataset['MonthlyIncome'], c=['purple'])
plt.xlabel('MonthlyIncome')
plt.ylabel('index')
plt.title('MonthlyIncome Scatter Plot')
plt.show()

"""*MonthlyIncome by JobLevel*"""

fig, ax = plt.subplots()
sns.regplot(x='JobLevel', 
                 y="MonthlyIncome", 
                 data=df_dataset, 
                 color='purple').set(title='MonthlyIncome by JobLevel') 
ax.set(xticks=np.arange(1, 6, 1))  
ax.set_xticklabels(['1', '2', '3','4','5'])

"""*MonthlyIncome by Education*"""

fig, ax = plt.subplots()
sns.regplot(x='Education', 
                 y="MonthlyIncome", 
                 data=df_dataset, 
                 color='purple').set(title='MonthlyIncome by Education') 
ax.set(xticks=np.arange(1, 6, 1))  
ax.set_xticklabels(['Below College', 'College', 'Bachelor','Master','Doctor'])
#plt.grid(b=None)

"""*MonthlyIncome by Age*"""

sns.regplot(x='Age', y="MonthlyIncome", data=df_dataset, color='purple').set(title='MonthlyIncome by Age');

sns.regplot(x='Age', y="JobLevel", data=df_dataset, color='purple').set(title='Age by JobLevel');

"""*PercentSalaryHike by PerformanceRating*"""

sns.regplot(x='PercentSalaryHike', y="PerformanceRating", data=df_dataset, color='purple').set(title='PercentSalaryHike by PerformanceRating');

"""making label df"""

labels =df_dataset['MonthlyIncome']
labels2 = labels
######
df_dataset.drop(['MonthlyIncome'], axis=1, inplace=True)
#df_dataset.describe

#labels
labelsDescribe = labels.describe(include='all')

"""JobLevel"""

# # ---------------------------- JobLevel  -----------------------------
uni = pd.value_counts(df_dataset['JobLevel'])
# print(uni)
# plot bar chart
p1 = sns.countplot(x=df_dataset['JobLevel'], color='purple')
p1.set(title='JobLevel Bar Chart')
plt.show()

"""DistanceFromHome"""

# histogram
sns.displot(data=df_dataset, x="DistanceFromHome", kde=True, color='purple').set(title='Distance From Home Density')
plt.show()
# scatter plot
plt.scatter(x=df_dataset.index, y=df_dataset['DistanceFromHome'], c=['purple'])
plt.xlabel('DistanceFromHome')
plt.ylabel('index')
plt.title('Distance From Home Scatter Plot')
plt.show()

"""Age"""

sns.distplot(df_dataset['Age'],  hist=False, kde=True, color='purple',norm_hist=True).set(title='Age Density')
plt.show()

"""EducationField"""

uni = pd.value_counts(df_dataset['EducationField'])
print(uni)
plt.figure(figsize=(10,6))
# plot bar chart
p1 = sns.countplot(x=df_dataset['EducationField'], color='purple')
p1.set(title='EducationField Bar Chart')
plt.show()

"""Education"""

uni = pd.value_counts(df_dataset['Education'])
print(uni)
plt.figure(figsize=(10,6))
# plot bar chart
p1 = sns.countplot(x=df_dataset['Education'], color='purple')
p1.set(title='Education Bar Chart')
p1.set_xticklabels(['Below College', 'College', 'Bachelor','Master','Doctor']) 
plt.show()

"""Gender"""

sns.countplot(x=df_dataset['Gender'], color='purple').set(title='Gender Bar Chart')
pd.value_counts(df_dataset['Gender'])

"""Job Satisfaction"""

sns.countplot(x=df_dataset['JobSatisfaction'], color='purple').set(title='JobSatisfaction Bar Chart')
pd.value_counts(df_dataset['JobSatisfaction'])

"""TotalWorkingYears"""

sns.displot(data=df_dataset, x="TotalWorkingYears", kde=True, color='purple').set(title='Total Working Years')

"""Attrition"""

sns.countplot(x=df_dataset['Attrition'], color='purple').set(title='Attrition')
pd.value_counts(df_dataset['Attrition'])

"""##### Check for Outliers"""

# boxplot
df_num = df_dataset.select_dtypes(exclude='object')
for i in range(len(df_num.columns)):
    sns.boxplot(df_num.iloc[:,i])
    plt.grid(b=None)
    plt.show()
    
# skewness
df_num.skew().sort_values(ascending = False)
df_dataset['YearsSinceLastPromotion'] = np.sqrt(df_dataset['YearsSinceLastPromotion'])
df_dataset['PerformanceRating'] = np.sqrt(df_dataset['PerformanceRating'])
df_dataset['YearsAtCompany'] = np.sqrt(df_dataset['YearsAtCompany'])

del df_num, i

"""cleaning the data"""

# check the same with pandas if there is duplication
rowsNum = len(df_dataset)
print('At the beginning df len if ', rowsNum)
df_dataset.drop_duplicates()
rowsNumNew = len(df_dataset)
print('After drop df len if ', rowsNumNew)
equal = 'Yes' if rowsNum == rowsNumNew else 'No'
print('Do the files equal? ' + equal)

# Check unique values - top 3 columns to remove
df_dataset.nunique().sort_values(ascending = True)

# Department-JobRole - unifing columns 
indexes = np.where(df_dataset['JobRole'] == 'Manager')
df_dataset['JobRole'].loc[indexes] = 'Manager of ' + df_dataset['Department'].loc[indexes]

del indexes

df_dataset.loc[df_dataset['YearsAtCompany'] >= 2, 'old_worker_indocator'] = 1
df_dataset.loc[df_dataset['YearsAtCompany'] < 2, 'old_worker_indocator'] = 0

# ------------------------------ delete ----------------------------------------
del df_dataset['Over18']
del df_dataset['StandardHours']
del df_dataset['EmployeeCount']
del df_dataset['PerformanceRating']   #we cant know the PerformanceRating before he started
del df_dataset['EmployeeNumber']
del df_dataset['DailyRate']
del df_dataset['HourlyRate']
del df_dataset['MonthlyRate']

# #see that the columns are gone
# dataDescribe = df_dataset.describe(include='all')
# print(dataDescribe)

"""convert to numeric no order"""

# ---------------------------convert to numeric no order----------------------------------------
df_dataset['Attrition'] = pd.factorize(df_dataset.Attrition)[0]
df_dataset['EducationField'] = pd.factorize(df_dataset.EducationField)[0]
df_dataset['Gender'] = pd.factorize(df_dataset.Gender)[0]
df_dataset['JobRole'] = pd.factorize(df_dataset.JobRole)[0]
df_dataset['MaritalStatus'] = pd.factorize(df_dataset.MaritalStatus)[0]
df_dataset['OverTime'] = pd.factorize(df_dataset.OverTime)[0]

"""drop 'Department'"""

# Department ~ JobRole - Because a position indicates the department we can connect them and drop the department
indexes = np.where(df_dataset['JobRole'] == 'Manager')
df_dataset['JobRole'].loc[indexes] = 'Manager of ' + df_dataset['Department'].loc[indexes]
del df_dataset['Department']

"""convert to numeric with order"""

# ---------------------------convert to numeric with order----------------------------------------
# label_encoder object knows how to understand word labels.
label_encoder = preprocessing.LabelEncoder()

# DistanceFromHome  0-30
df_dataset['catDistanceFromHome'] = pd.cut(df_dataset.DistanceFromHome,bins=[-1,10,15,20,25,30],labels=[1,2,3,4,5])
del df_dataset['DistanceFromHome']

# TotalWorkingYears 0-40
df_dataset['catTotalWorkingYears'] = pd.cut(df_dataset.TotalWorkingYears,bins=[-1,10,15,20,25,50],labels=[1,2,3,4,5])
del df_dataset['TotalWorkingYears']

# YearsAtCompany  0-40
df_dataset['catYearsAtCompany'] = pd.cut(df_dataset.YearsAtCompany,bins=[-1,10,15,20,25,50],labels=[1,2,3,4,5])
del df_dataset['YearsAtCompany']

# YearsInCurrentRole  0-18 
df_dataset['catYearsInCurrentRole'] = pd.cut(df_dataset.YearsInCurrentRole,bins=[-1,10,20,30],labels=[1,2,3])
del df_dataset['YearsInCurrentRole']

# YearsWithCurrManager  0-17
df_dataset['catYearsWithCurrManager'] = pd.cut(df_dataset.YearsWithCurrManager,bins=[-1,6,10,30],labels=[1,2,3])
del df_dataset['YearsWithCurrManager']

# YearsSinceLastPromotion  0-15 
df_dataset['catYearsSinceLastPromotion'] = pd.cut(df_dataset.YearsSinceLastPromotion,bins=[-1,5,10,15,20,30],labels=[1,2,3,4,5])
del df_dataset['YearsSinceLastPromotion']

# BusinessTravel  'Non-Travel', 'Travel_Rarely', 'Travel_Frequently' 
a = pd.Categorical( df_dataset['BusinessTravel'], categories =['Non-Travel', 'Travel_Rarely', 'Travel_Frequently'])
df_dataset['BusinessTravel'] = pd.factorize(a)[0]

"""Normalization"""

temp = df_dataset.columns

df_dataset = preprocessing.MinMaxScaler().fit_transform(df_dataset)
df_dataset = pd.DataFrame(df_dataset, columns=temp)

del(temp)

"""new features"""

df_dataset['worker'] =0.2 * df_dataset['JobInvolvement'] +0.2 * df_dataset['PercentSalaryHike'] +0.2 * df_dataset['TrainingTimesLastYear']  +0.4 * df_dataset['catYearsSinceLastPromotion']

df_dataset['Satisfaction'] =  0.3 * df_dataset['JobSatisfaction'] + 0.2 * df_dataset['EnvironmentSatisfaction'] + 0.3 * df_dataset['catYearsWithCurrManager']+ 0.1 * df_dataset['catYearsSinceLastPromotion']+ 0.1 * df_dataset['RelationshipSatisfaction']

df_dataset['worker'] =  df_dataset['worker']  * df_dataset['old_worker_indocator'] 
df_dataset['Satisfaction'] =  df_dataset['Satisfaction']  * df_dataset['old_worker_indocator']

"""heat map"""

# Correlation Map
plt.subplots(figsize=(16,16))
sns.heatmap(df_dataset.corr(), annot=True, cmap='plasma')
plt.title("Correlation Matrix: Heat Map", fontsize=20)
plt.show()

"""Calculation of high correlations larger than 0.4 ans smaller than -0.4"""

def all_correlation(correlation_df):
    result_names = []
    result = []
    
    for i in correlation_df.columns:
        # target variable
        if i == 'MonthlyIncome':
            continue
        for j in correlation_df.index:
            # featue with itself
            if i == j:
                continue
            # low correlation    
            elif correlation_df.at[i, j] <= 0.4 and correlation_df.at[i, j] >= -0.4:
                continue
            # filter uniques
            elif (j, i) in result_names:
                continue
            # target variable    
            elif j == 'MonthlyIncome':
                continue
            else:
                result_names.append((i, j))
                result.append(correlation_df.at[i, j])
    
    
    return pd.DataFrame(list(zip(result_names, result)), 
                        columns =['Pair', 'Corr']).sort_values('Corr', ascending = False)


all_correlation(df_dataset.corr()).head(20)

"""Deleting features"""

del df_dataset['old_worker_indocator']
del df_dataset['Satisfaction']
####
del df_dataset['Age'] 
del df_dataset['MaritalStatus']
del df_dataset['catYearsAtCompany']
del df_dataset['catTotalWorkingYears']
######
del df_dataset['JobSatisfaction']
del df_dataset['EnvironmentSatisfaction']
del df_dataset['JobInvolvement']
del df_dataset['RelationshipSatisfaction']
del df_dataset['TrainingTimesLastYear']
del df_dataset['catYearsSinceLastPromotion']
del df_dataset['catYearsWithCurrManager']
del df_dataset['catYearsInCurrentRole']
####

dataDescribe = df_dataset.describe(include='all')
print(dataDescribe)

print(df_dataset.columns)
df_dataset_imp = df_dataset

import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler

"""# k-means"""

from sklearn.cluster import KMeans
from sklearn.metrics.cluster import homogeneity_score
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, davies_bouldin_score

iner_list = []
dbi_list = []
sil_list = []

datasetKmeans = df_dataset
#kmeans = KMeans(n_clusters=5, max_iter=300, n_init=10, random_state=42)
from tqdm import tqdm
for n_clusters in tqdm(range(2, 10, 1)):
    kmeans = KMeans(n_clusters=n_clusters, max_iter=300, n_init=10, random_state=42)
    kmeans.fit(datasetKmeans)
    assignment = kmeans.predict(datasetKmeans)
    
    iner = kmeans.inertia_
    sil = silhouette_score(datasetKmeans, assignment)
    dbi = davies_bouldin_score(datasetKmeans, assignment)
    
    dbi_list.append(dbi)
    sil_list.append(sil)
    iner_list.append(iner)

plt.plot(range(2, 10, 1), iner_list, marker='o', color='c')
plt.title("Inertia")
plt.xlabel("Number of clusters")
#plt.grid(b=None)
plt.show()

plt.plot(range(2, 10, 1), sil_list, marker='o',color='orange')
plt.title("Silhouette")
plt.xlabel("Number of clusters")
#plt.grid(b=None)
plt.show()

plt.plot(range(2, 10, 1), dbi_list, marker='o',color='m')
plt.title("Davies-Bouldin")
plt.xlabel("Number of clusters")
#plt.grid(b=None)
plt.show()

"""*Split - train-test*



"""

X_train, X_test, y_train, y_test = train_test_split(df_dataset, labels, test_size=0.2, random_state = 0)

"""#**Linear Regression**"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
from sklearn.linear_model import LinearRegression

# %time reg = LinearRegression().fit(X_train, y_train)
print(reg.score(X_train, y_train))

print('cof', reg.coef_)

reg.intercept_

y_hat_REG = reg.predict(X_test)


print('Basic LinearRegression')
print('explained_variance_score:', explained_variance_score(y_test, y_hat_REG))
print('RMSE:', mean_squared_error(y_test, y_hat_REG, squared=False))
print('R2:', r2_score(y_test, y_hat_REG))

"""# assumptions associated with a linear regression model"""

import pandas as pd
import numpy as np
from sklearn import datasets, linear_model
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm
from scipy import stats

X = X_train
y = y_train

X2 = sm.add_constant(X) 
est = sm.OLS(y, X2)
est2 = est.fit()
print(est2.summary())

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import scipy as sp
import seaborn as sns
import statsmodels.api as sm
import statsmodels.tsa.api as smt
import warnings
from google.colab import drive
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from statsmodels.stats.outliers_influence import variance_inflation_factor 
X_with_constant = sm.add_constant(X_train)
model = sm.OLS(y_train, X_with_constant)
results = model.fit()
results.params

X = X_test
y = y_test

X = sm.add_constant(X)
y_pred_lin = results.predict(X)
residual = y_test - y_pred_lin

"""**No Multicolinearity**"""

vif = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]
pd.DataFrame({'vif': vif[0:]}, index=X_train.columns).T

"""** **bold text**Normailty of Residual**"""

sns.distplot(residual)

fig, ax = plt.subplots(figsize=(6,2.5))
_, (__, ___, r) = sp.stats.probplot(residual, plot=ax, fit=True)


print(np.mean(residual))

"""**Homoscedasticity**"""

fig, ax = plt.subplots(figsize=(6,2.5))
_ = ax.scatter(y_pred_lin, residual)

"""**No autocorrelation of residuals**"""

acf = smt.graphics.plot_acf(residual, lags=40 , alpha=0.05)
acf.show()

"""# **Ridge**"""

import pandas as pd
import numpy as np
from sklearn import datasets, linear_model
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm
from scipy import stats

# List to maintain the different cross-validation scores
cross_val_scores_ridge = []
 
# List to maintain the different values of alpha
alpha = []
 
# Loop to compute the different values of cross-validation scores
for i in range(0, 9):
    ridgeModel = linear_model.Ridge(alpha = i * 0.25)
    ridgeModel.fit(X_train, y_train)
    scores = cross_val_score(ridgeModel, X, y, cv = 10)
    avg_cross_val_score = ((scores)*100).mean()
    cross_val_scores_ridge.append(avg_cross_val_score)
    alpha.append(i * 0.25)
 
# Loop to print the different values of cross-validation scores
print('Ridge - avg cross val score for different alpha')
for i in range(0, len(alpha)):
    print(str(alpha[i])+' : '+str(cross_val_scores_ridge[i]))

# Building and fitting the Ridge Regression Model
RidgeoModelChosen = linear_model.Ridge(alpha = 0.25)
RidgeoModelChosen.fit(X_train, y_train)
 
# Evaluating the Ridge Regression model
print(RidgeoModelChosen.score(X_test, y_test))
predictionsRidge = RidgeoModelChosen.predict(X_test)

print('Ridge')
print('explained_variance_score:', explained_variance_score(y_test, predictionsRidge))
print('RMSE:', mean_squared_error(y_test, predictionsRidge, squared=False))
print('R2:', r2_score(y_test, predictionsRidge))
print('MAE:', mean_absolute_error(y_test, predictionsRidge))

"""# **Lasso**"""

import pandas as pd
import numpy as np
from sklearn import datasets, linear_model
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm
from scipy import stats
from sklearn import linear_model
# List to maintain the cross-validation scores
cross_val_scores_lasso = []
 
# List to maintain the different values of Lambda
Lambda = []
 
# Loop to compute the cross-validation scores
for i in range(0, 9):
    lassoModel = linear_model.Lasso(alpha = i * 0.25, tol = 0.0925)
    lassoModel.fit(X_train, y_train)
    scores = cross_val_score(lassoModel, X, y, cv = 10)
    avg_cross_val_score = ((scores)*100).mean()
    cross_val_scores_lasso.append(avg_cross_val_score)
    Lambda.append(i * 0.25)
 
# Loop to print the different values of cross-validation scores
print('Lasso - avg cross val score for different lambda ')
for i in range(0, len(alpha)):
    print(str(alpha[i])+' : '+str(cross_val_scores_lasso[i]))

# Building and fitting the Lasso Regression Model
lassoModelChosen = linear_model.Lasso(alpha = 2.0, tol = 0.0925)
lassoModelChosen.fit(X_train, y_train)
 
# Evaluating the Lasso Regression model
print(lassoModelChosen.score(X_test, y_test))
predictionsLasso = lassoModelChosen.predict(X_test)

print('Lasso')
print('explained_variance_score:', explained_variance_score(y_test, predictionsLasso))
print('RMSE:', mean_squared_error(y_test, predictionsLasso, squared=False))
print('R2:', r2_score(y_test, predictionsLasso))
print('MAE:', mean_absolute_error(y_test, predictionsLasso))

# Commented out IPython magic to ensure Python compatibility.
from sklearn import linear_model
reg = linear_model.Lasso(alpha=0.1)
# %time reg.fit(X_train, y_train)

predictionsLasso = reg.predict(X_test)

print('Basic Lasso')
print('explained_variance_score:', explained_variance_score(y_test, predictionsLasso))
print('RMSE:', mean_squared_error(y_test, predictionsLasso, squared=False))
print('R2:', r2_score(y_test, predictionsLasso))
print('MAE:', mean_absolute_error(y_test, predictionsLasso))

"""# **Random Forest BASIC**"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn import metrics
from sklearn.metrics import explained_variance_score
from sklearn.metrics import mean_squared_error
#Create a Gaussian Classifier
clf=RandomForestRegressor(n_estimators=100)
#Train the model using the training sets y_pred=clf.predict(X_test)
# %time clf.fit(X_train,y_train)
y_pred=clf.predict(X_test)
#
print('Basic RandomForestRegressor')
print('explained_variance_score:', explained_variance_score(y_test, y_pred))
print('RMSE:', mean_squared_error(y_test, y_pred, squared=False))
print('R2:', r2_score(y_test, y_pred))
print('MAE:', mean_absolute_error(y_test, y_pred))

"""# **Random Forest + CV**"""

# Commented out IPython magic to ensure Python compatibility.
grid_searchRF = GridSearchCV(estimator=RandomForestRegressor(random_state=0),
                           param_grid={ 
                                       'bootstrap': [True],
                                        'max_depth': [50,70,90],
                                        'min_samples_split': [2, 5, 10, 20, 50],
                                        'n_estimators': [100, 200, 300]
                                        },
                           refit=True,
                           cv=10) 
# %time grid_searchRF.fit(X_train, y_train)
cv_best_RF = grid_searchRF.best_estimator_ 
print(grid_searchRF.best_params_)
cv_FR_results = cross_validate(cv_best_RF, X_train, y_train, cv=10, return_train_score=True)
print('cv_FR_results',cv_FR_results)
y_pred_RF=cv_best_RF.predict(X_test)
#List of features for later use
feature_list = list(X_train.columns)
# Get numerical feature importances
importances = list(cv_best_RF.feature_importances_)
# List of tuples with variable and importance
feature_importances = [(feature, round(importance, 10)) for feature, importance in zip(feature_list, importances)]
# Sort the feature importances by most important first
feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)
# Print out the feature and importances 
[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]

# print(df_dataset.columns)

print('Improved RandomForestRegressor')
print('explained_variance_score:', explained_variance_score(y_test, y_pred_RF))
print('RMSE:', mean_squared_error(y_test, y_pred_RF, squared=False))
print('R2:', r2_score(y_test, y_pred_RF))
print('MAE:', mean_absolute_error(y_test, y_pred_RF))

"""# **MLP BASIC**"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.neural_network import MLPRegressor

# %time regr = MLPRegressor().fit(X_train, y_train)
y_pred = regr.predict(X_test)
#print(regr.score(X_test, y_test))
#
print('Basic MLPRegressor')
print('explained_variance_score:', explained_variance_score(y_test, y_pred))
print('RMSE:', mean_squared_error(y_test, y_pred, squared=False))
print('R2:', r2_score(y_test, y_pred))
print('MAE:', mean_absolute_error(y_test, y_pred))

"""# **MLP + CV**"""

# Commented out IPython magic to ensure Python compatibility.
from sklearn.neural_network import MLPRegressor
from sklearn.datasets import make_regression
from sklearn.model_selection import GridSearchCV
from sklearn import metrics
from sklearn.metrics import mean_squared_error
from sklearn.metrics import explained_variance_score
from sklearn.metrics import r2_score

mlp_param_grid = {
            'activation': ['identity', 'logistic' 'relu', 'tanh'],
            'solver' :   [ 'lbfgs' ] , 
            'learning_rate' : ['constant', 'invscaling', 'adaptive'],
            'learning_rate_init': np.arange(0.0001,0.1),
            'alpha' :  np.arange(0.0,0.0001,0.00001),
            'hidden_layer_sizes': [(14, ), (28, )]
                  }

mlp_grid_search = GridSearchCV(estimator=MLPRegressor(max_iter=200),
                           param_grid=mlp_param_grid,
                           refit=True,
                           cv=10)

# %time mlp_grid_search.fit(X_train, y_train)
best_mlp_estimator = mlp_grid_search.best_estimator_; best_mlp_estimator
# %time best_mlp_estimator.fit(X_train, y_train)
best_mlp_estimator.get_params(deep=True)
y_pred_MLP = best_mlp_estimator.predict(X_test) 
#
print(best_mlp_estimator)
#
print('Improved MLPRegressor')
print('explained_variance_score:', explained_variance_score(y_test, y_pred_MLP))
print('RMSE:', mean_squared_error(y_test, y_pred_MLP, squared=False))
print('R2:', r2_score(y_test, y_pred_MLP))
print('MAE:', mean_absolute_error(y_test, y_pred_MLP))

print('Improved MLPRegressor')
print('explained_variance_score:', explained_variance_score(y_test, y_pred_MLP))
print('RMSE:', mean_squared_error(y_test, y_pred_MLP, squared=False))
print('R2:', r2_score(y_test, y_pred_MLP))
print('MAE:', mean_absolute_error(y_test, y_pred_MLP))

print(best_mlp_estimator)

"""# **XGBOOST BASIC**"""

# Commented out IPython magic to ensure Python compatibility.
import xgboost as xgb
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error
# define model
model = XGBRegressor()
# fit model
arr1 = X_train.to_numpy()
arr2 = y_train.to_numpy()
arr3 = X_test.to_numpy()
# %time model.fit(arr1 , arr2)
# make a prediction
yhat = model.predict(arr3)
# summarize prediction
print(model.get_params)

print('Basic XGBRegressor')
print('explained_variance_score:', explained_variance_score(y_test, yhat))
print('RMSE:', mean_squared_error(y_test, yhat, squared=False ))
print('R2:', r2_score(y_test, yhat))
print('MAE:', mean_absolute_error(y_test, yhat))

"""# **XGBOOST  + CV**"""

# Commented out IPython magic to ensure Python compatibility.
import xgboost as xgb
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error
X_trainaa = X_train.to_numpy()
X_trainbb = X_test.to_numpy()
arr1 = xgb.DMatrix(X_trainaa, label = y_train)
arr2 = xgb.DMatrix(X_trainbb, label = y_test)
print(X_trainaa)
estimator = xgb.XGBRegressor()
parameters = {'max_depth':[3,6,9]  ,
              'min_child_weight': [1,2],
              'eta':[0.3,0.5],
              'subsample': [1,2],
              'colsample_bytree': [1,2]
}



grid_search_XG = GridSearchCV(
    estimator=estimator,
    param_grid=parameters,
    scoring = 'r2',
    n_jobs = 2,
    cv = 10,    
    verbose=True
    )

arr3 = X_test.to_numpy()
# %time grid_search_XG.fit(X_trainaa,y_train)
print(grid_search_XG.best_estimator_)
preds_XG = grid_search_XG.predict(arr3)
# print('Predicted: %.3f' , preds_XG)
# MSE = mean_squared_error(y_test , preds_XG)
# print('XGBoost validation MSE = ',MSE)
print(grid_search_XG)

#
print('Improved XGBRegressor')
print('explained_variance_score:', explained_variance_score(y_test, preds_XG))
print('RMSE:', mean_squared_error(y_test, preds_XG, squared=False ))
print('R2:', r2_score(y_test, preds_XG))
print('MAE:', mean_absolute_error(y_test, preds_XG))

"""# compare the models prediction"""

plt.figure(figsize=(10,6))
plt.plot(y_hat_REG, 'yo', label='LinearRegression')
plt.plot(y_pred_RF, 'b^', label='RandomForestRegressor')
plt.plot(y_pred_MLP, 'gs', label='MLPRegression')
plt.plot(preds_XG, 'r*', ms=10, label='XGBRegressor')


plt.tick_params(axis='x', which='both', bottom=False, top=False,
                labelbottom=False)
plt.ylabel('predicted')
plt.xlabel('X_test samples')
plt.legend(loc="best")
plt.title('Regressor predictions and their average')
plt.grid(b=None)
plt.show()

"""You can open a different row each time to see the relationships between the results of the model runs"""

plt.figure(figsize=(10,6))
# plt.plot(predictions2, 'yo', label='LinearRegression')
# plt.plot(y_pred_RF, 'b^', label='RandomForestRegressor')
plt.plot(y_pred_MLP, 'gs', label='MLPRegression')
# plt.plot(preds_XG, 'r*', ms=10, label='XGBRegressor')

plt.tick_params(axis='x', which='both', bottom=False, top=False,
                labelbottom=False)
plt.ylabel('predicted')
plt.xlabel('training samples')
plt.legend(loc="best")
plt.title('Regressor predictions and their average')
plt.grid(b=None)
plt.show()

"""# Comparing all performance of the models"""

print('RMSE_XG:', mean_squared_error(y_test, preds_XG, squared=False ))
print('RMSE_LINEAR:', mean_squared_error(y_test, y_hat_REG, squared=False ))
print('RMSE_RF:', mean_squared_error(y_test, y_pred_RF, squared=False ))
print('RMSE_MLP:', mean_squared_error(y_test, y_pred_MLP, squared=False ))
print('***********************************************')
print('R2xg:', r2_score(y_test, preds_XG))
print('R2linear:', r2_score(y_test, y_hat_REG))
print('R2rf:', r2_score(y_test, y_pred_RF))
print('R2mlp:', r2_score(y_test, y_pred_MLP))
print('***********************************************')
print('explained_variance_scorexg:', explained_variance_score(y_test, preds_XG))
print('explained_variance_scorelinear:', explained_variance_score(y_test, y_hat_REG))
print('explained_variance_scorerf:', explained_variance_score(y_test, y_pred_RF))
print('explained_variance_scoremlp:', explained_variance_score(y_test, y_pred_MLP))
print('***********************************************')
print('MAExg:', mean_absolute_error(y_test, preds_XG))
print('MAEline:', mean_absolute_error(y_test, y_hat_REG))
print('MAErf:', mean_absolute_error(y_test, y_pred_RF))
print('MAEmlp:', mean_absolute_error(y_test, y_pred_MLP))

"""# **Improvement**"""

lastpred = ((y_pred_RF+y_pred_MLP+preds_XG)/3)

print('New Pred')
print('RMSE:', mean_squared_error(y_test, lastpred, squared=False ))
print('R2:', r2_score(y_test, lastpred))
print('explained_variance_scoremlp:', explained_variance_score(y_test, lastpred))
print('MAE:', mean_absolute_error(y_test, lastpred))

"""# For enrichment - not mentioned in the report

# NN- 5 lyers

>
"""

from keras.callbacks import ModelCheckpoint
from keras.models import Sequential
from keras.layers import Dense, Activation, Flatten
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error 
from matplotlib import pyplot as plt
import seaborn as sb
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import warnings 
warnings.filterwarnings('ignore')
warnings.filterwarnings('ignore', category=DeprecationWarning)
from xgboost import XGBRegressor

NN_model = Sequential()

# The Input Layer :
NN_model.add(Dense(26, kernel_initializer='normal',input_dim = X_train.shape[1], activation='relu'))

# The Hidden Layers :
NN_model.add(Dense(26, kernel_initializer='normal',activation='relu'))
NN_model.add(Dense(26, kernel_initializer='normal',activation='relu'))
NN_model.add(Dense(26, kernel_initializer='normal',activation='relu'))

# The Output Layer :
NN_model.add(Dense(1, kernel_initializer='normal',activation='linear'))

# Compile the network :
NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])
NN_model.summary()

checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' 
checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')
callbacks_list = [checkpoint]

print(checkpoint)

# Commented out IPython magic to ensure Python compatibility.
# %time NN_model.fit(X_train, y_train, epochs=500, batch_size=32, validation_split = 0.2, callbacks=callbacks_list)

import os
import datetime as dt
import six
import numpy as np
import h5py

hdFileName = 'Weights-331--964.97888.hdf5'
modeType   = 'r'
hdfid = h5py.File(hdFileName, modeType)

print(hdfid)

wights_file = hdFileName

NN_model.load_weights(wights_file) # load it
NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])

predictions = NN_model.predict(X_test)
# print(predictions)

print(mean_squared_error(y_test, predictions))
print(mean_squared_error(y_test, predictions, squared=False))
print(r2_score(y_test, predictions))
print(explained_variance_score(y_test, predictions))